{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uilA3K18JbHP",
    "outputId": "8efec4bd-48d1-4a66-dc2b-7c6810275a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Using cached praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Using cached prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Using cached update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting websocket-client>=0.54.0 (from praw)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests<3.0,>=2.6.0 (from prawcore<3,>=2.4->praw)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw)\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Using cached prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Using cached update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: websocket-client, urllib3, idna, charset-normalizer, certifi, requests, update_checker, prawcore, praw\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 praw-7.8.1 prawcore-2.4.0 requests-2.32.3 update_checker-0.18.0 urllib3-2.2.3 websocket-client-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZH_dr-UXJhT9"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpraw\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import yfinance as yf  # Import yfinance to fetch beta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCthVUm7Jkz4",
    "outputId": "6c57f1f6-ea02-4865-9b90-8f9c11eeac77"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_CPP_MIN_LOG_LEVEL\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Suppress TensorFlow logs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Suppress NLTK download output\u001b[39;00m\n\u001b[1;32m      4\u001b[0m original_stdout \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mstdout\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logs\n",
    "\n",
    "# Suppress NLTK download output\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = io.StringIO()\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "sys.stdout = original_stdout\n",
    "\n",
    "# Suppress other library logs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================\n",
    "# 1. Setup and Load Pre-trained BERT Model\n",
    "# =============================================\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# =============================================\n",
    "# 2. Define Sentiment Prediction Function\n",
    "# =============================================\n",
    "\n",
    "def predict_sentiment(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a given text using a pre-trained BERT model.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "        tokenizer: The BERT tokenizer.\n",
    "        model: The pre-trained BERT model.\n",
    "        device: The device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Sentiment Label, Confidence Score)\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return 'neutral', 0.0\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        confidence, predicted_class = torch.max(probabilities, dim=1)\n",
    "\n",
    "    # Map numerical labels to sentiment\n",
    "    label_mapping = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "    sentiment = label_mapping.get(predicted_class.item(), 'Neutral')\n",
    "    confidence_score = confidence.item()\n",
    "\n",
    "    return sentiment, confidence_score\n",
    "\n",
    "# =============================================\n",
    "# 3. Reddit Sentiment Analysis Function\n",
    "# =============================================\n",
    "\n",
    "def reddit_sentiment_analysis(client_id, client_secret, user_agent, stock_ticker, company_name, subreddits, reddit_limit=100):\n",
    "    \"\"\"\n",
    "    Fetches Reddit posts related to the stock and performs sentiment analysis.\n",
    "\n",
    "    Parameters:\n",
    "        client_id (str): Reddit API client ID.\n",
    "        client_secret (str): Reddit API client secret.\n",
    "        user_agent (str): Reddit API user agent.\n",
    "        stock_ticker (str): Stock ticker symbol (e.g., 'RELIANCE.NS').\n",
    "        company_name (str): Full company name (e.g., 'Reliance Industries').\n",
    "        subreddits (list): List of subreddit names to search.\n",
    "        reddit_limit (int): Number of posts to fetch per query per subreddit.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DataFrame containing comments and adjusted scores, average adjusted score)\n",
    "    \"\"\"\n",
    "    # Initialize Reddit instance\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "\n",
    "    queries = [f\"${stock_ticker.split('.')[0]}\", company_name]\n",
    "    posts = []\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        for query in queries:\n",
    "            print(f\"Searching in r/{subreddit} for '{query}'...\")\n",
    "            try:\n",
    "                for submission in reddit.subreddit(subreddit).search(query, limit=reddit_limit, sort='new'):\n",
    "                    posts.append({\n",
    "                        'Combined_Text': f\"{submission.title} {submission.selftext}\"\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching posts from r/{subreddit} with query '{query}': {e}\")\n",
    "                continue\n",
    "\n",
    "    reddit_df = pd.DataFrame(posts)\n",
    "    print(f\"\\nFetched {len(reddit_df)} Reddit posts.\\n\")\n",
    "\n",
    "    # Perform sentiment analysis\n",
    "    adjusted_scores = []\n",
    "\n",
    "    for text in tqdm(reddit_df['Combined_Text'], desc=\"Analyzing Reddit Sentiments\", disable=True):\n",
    "        sentiment, confidence = predict_sentiment(text, tokenizer, model, device)\n",
    "\n",
    "        # Assign numeric value based on sentiment\n",
    "        if sentiment == 'Positive':\n",
    "            sentiment_value = -1\n",
    "        elif sentiment == 'Neutral':\n",
    "            sentiment_value = 0\n",
    "        else:  # Negative\n",
    "            sentiment_value = 1\n",
    "\n",
    "        # Calculate adjusted score\n",
    "        adjusted_score = sentiment_value * confidence\n",
    "        adjusted_scores.append(adjusted_score)\n",
    "\n",
    "    # Add adjusted scores to DataFrame\n",
    "    reddit_df['Adjusted_Score'] = adjusted_scores\n",
    "\n",
    "    # Calculate the average of adjusted scores\n",
    "    average_adjusted_score = reddit_df['Adjusted_Score'].mean()\n",
    "\n",
    "    # Filter to only keep the relevant column\n",
    "    reddit_output_df = reddit_df[['Combined_Text', 'Adjusted_Score']]\n",
    "\n",
    "    # Save to CSV\n",
    "    reddit_output_df.to_csv('reddit_stock_sentiment_adjusted_scores.csv', index=False)\n",
    "    print(\"Reddit sentiment results saved to 'reddit_stock_sentiment_adjusted_scores.csv'.\\n\")\n",
    "\n",
    "    return reddit_output_df, average_adjusted_score\n",
    "\n",
    "# =============================================\n",
    "# 4. YouTube Sentiment Analysis Function\n",
    "# =============================================\n",
    "\n",
    "def youtube_sentiment_analysis(api_key, stock_symbol, company_name, youtube_limit=5, comments_limit=100):\n",
    "    \"\"\"\n",
    "    Fetches YouTube comments related to the stock and performs sentiment analysis.\n",
    "\n",
    "    Parameters:\n",
    "        api_key (str): YouTube Data API key.\n",
    "        stock_symbol (str): Stock symbol (e.g., 'RELIANCE.NS').\n",
    "        company_name (str): Full company name (e.g., 'Reliance Industries').\n",
    "        youtube_limit (int): Number of videos to fetch per query.\n",
    "        comments_limit (int): Number of comments to fetch per video.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing comments and their sentiment scores.\n",
    "    \"\"\"\n",
    "    search_queries = [f\"{stock_symbol.split('.')[0]}\", company_name]\n",
    "    video_ids = []\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    for query in search_queries:\n",
    "        print(f\"Searching YouTube for '{query}'...\")\n",
    "        try:\n",
    "            search_response = youtube.search().list(\n",
    "                q=query,\n",
    "                part='id',\n",
    "                type='video',\n",
    "                maxResults=youtube_limit,\n",
    "                order='date'  # Fetch recent videos\n",
    "            ).execute()\n",
    "\n",
    "            vids = [item['id']['videoId'] for item in search_response.get('items', [])]\n",
    "            video_ids.extend(vids)\n",
    "            print(f\"Found {len(vids)} videos for query '{query}'.\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching YouTube for query '{query}': {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Total videos found: {len(video_ids)}\\n\")\n",
    "\n",
    "    # Fetch comments for each video\n",
    "    all_comments = []\n",
    "    for vid in tqdm(video_ids, desc=\"Fetching YouTube Comments\", disable=True):\n",
    "        try:\n",
    "            comment_response = youtube.commentThreads().list(\n",
    "                videoId=vid,\n",
    "                part='snippet',\n",
    "                maxResults=comments_limit,\n",
    "                textFormat='plainText',\n",
    "                order='relevance'\n",
    "            ).execute()\n",
    "\n",
    "            for item in comment_response.get('items', []):\n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                # Check if the comment mentions the stock symbol or company name\n",
    "                if stock_symbol.split('.')[0].lower() in comment.lower() or company_name.lower() in comment.lower():\n",
    "                    sentiment, confidence = predict_sentiment(comment, tokenizer, model, device)\n",
    "                    all_comments.append({\n",
    "                        'Comment': comment,\n",
    "                        'Sentiment': sentiment,\n",
    "                        'Confidence': confidence\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for video {vid}: {e}\")\n",
    "            continue\n",
    "        # To respect API rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    youtube_df = pd.DataFrame(all_comments)\n",
    "    print(f\"\\nFetched and analyzed {len(youtube_df)} relevant YouTube comments.\\n\")\n",
    "\n",
    "    # Save to CSV\n",
    "    youtube_df.to_csv('youtube_stock_sentiment.csv', index=False)\n",
    "    print(\"YouTube sentiment results saved to 'youtube_stock_sentiment.csv'.\\n\")\n",
    "\n",
    "    return youtube_df\n",
    "\n",
    "# =============================================\n",
    "# 5. Main Function\n",
    "# =============================================\n",
    "\n",
    "def for_one_stock(lele,lelecompany_name):\n",
    "    # ----------------------------------------\n",
    "    # User Inputs\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Reddit API Credentials\n",
    "    reddit_client_id = 'YS3tFotOu28fhCV6qbn9Ag'          # Replace with your client ID\n",
    "    reddit_client_secret = 'XNgpl9XAxCo326LKUgYKYFMK40o0eA' # Replace with your client secret\n",
    "    reddit_user_agent = 'stock_sentiment_analysis by /u/Comfortable-Title817'  # Replace with your Reddit username\n",
    "\n",
    "    # YouTube API Key\n",
    "    youtube_api_key = 'AIzaSyD-QvEK0JfHRzqYdlympWADm3yVuJj32Gg'  # Replace with your actual YouTube Data API key\n",
    "\n",
    "    # Stock Information\n",
    "    stock_ticker = lele # Example: RELIANCE Industries\n",
    "    company_name = lelecompany_name\n",
    "\n",
    "    # Subreddits to search\n",
    "    subreddits = ['stocks', 'investing', 'IndianStockMarket']\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Reddit Sentiment Analysis\n",
    "    # ----------------------------------------\n",
    "\n",
    "    reddit_results, average_sentiment_score = reddit_sentiment_analysis(\n",
    "        client_id=reddit_client_id,\n",
    "        client_secret=reddit_client_secret,\n",
    "        user_agent=reddit_user_agent,\n",
    "        stock_ticker=stock_ticker,\n",
    "        company_name=company_name,\n",
    "        subreddits=subreddits,\n",
    "        reddit_limit=10  # Number of posts per query per subreddit\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # YouTube Sentiment Analysis (Optional)\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Uncomment the following lines if you want to perform YouTube sentiment analysis\n",
    "    youtube_results = youtube_sentiment_analysis(\n",
    "        api_key=youtube_api_key,\n",
    "        stock_symbol=stock_ticker,\n",
    "        company_name=company_name,\n",
    "        youtube_limit=5,    # Number of videos per query\n",
    "        comments_limit=10   # Number of comments per video\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Fetch Beta Value from yFinance\n",
    "    # ----------------------------------------\n",
    "\n",
    "    try:\n",
    "        ticker_obj = yf.Ticker(stock_ticker)\n",
    "        beta = ticker_obj.info.get('beta', None)\n",
    "        if beta is None:\n",
    "            print(\"Beta value not found for the stock.\")\n",
    "            beta = 0.0  # Assign a default value or handle accordingly\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching beta from yFinance: {e}\")\n",
    "        beta = 0.0  # Assign a default value or handle accordingly\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Calculate Final Risk Score\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Change the sign of the sentiment score\n",
    "    adjusted_sentiment_score = -average_sentiment_score\n",
    "\n",
    "    # Calculate weighted average: 80% beta and 20% adjusted sentiment score\n",
    "    final_risk_score = 0.9 * beta + 0.1 * adjusted_sentiment_score\n",
    "\n",
    "    # Determine if it's an increase or decrease\n",
    "    if final_risk_score > 0:\n",
    "        risk_trend = \"Increase\"\n",
    "    elif final_risk_score < 0:\n",
    "        risk_trend = \"Decrease\"\n",
    "    else:\n",
    "        risk_trend = \"No Change\"\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Print Final Risk Score\n",
    "    # ----------------------------------------\n",
    "\n",
    "    print(f\"\\nFinal Risk Score for {stock_ticker}: {final_risk_score:.2f}\")\n",
    "    print(f\"\\nReddit Results {reddit_results} , Youtube Results {youtube_results}\")\n",
    "\n",
    "    # Optionally, you can merge or further process these results as needed\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# for_one_stock('RELIANCE.NS','Reliance Industries')\n",
    "# Add this code at the end of your existing file, after the for_one_stock function\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Function to fetch company name from ticker\n",
    "def get_company_name(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        return stock.info.get('longName', 'Unknown')  # Returns the company name\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching company name for {ticker}: {str(e)}\")\n",
    "        return 'Unknown'\n",
    "\n",
    "def add_company_names(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Add company names to the input CSV based on ticker symbols and save the results to output CSV.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV containing recommended stocks\n",
    "        output_csv_path (str): Path to save the output CSV with company names added\n",
    "    \"\"\"\n",
    "    # Read the input CSV\n",
    "    print(f\"Reading stocks from {input_csv_path}\")\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Add the 'Company_Name' column\n",
    "    df['Company_Name'] = df['Ticker'].apply(get_company_name)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV\n",
    "    print(f\"Saving results to {output_csv_path}\")\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(\"Done!\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = \"/content/recommended_stocks.csv\"  # Path to your input CSV file\n",
    "output_csv_path = \"output_stocks_with_names.csv\"  # Path to save the new CSV\n",
    "add_company_names(input_csv_path, output_csv_path)\n",
    "for_one_stock('RELIANCE.NS','Reliance Industries')\n",
    "\n",
    "# # Add this at the very end of your file\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_path = \"/content/recommended_stocks.csv\"  # Path to your input CSV\n",
    "#     output_path = \"recommended_stocks_with_risk.csv\"  # Path for output CSV\n",
    "\n",
    "#     calculate_risk_scores(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA11n3VH-r4r"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
