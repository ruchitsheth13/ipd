{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45fece5e-35c1-461b-8b68-8fda72a22b68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing data...\n",
      "Loading the latest diversification metrics...\n",
      "Preprocessing completed.\n",
      "Optimal number of clusters determined: 2\n",
      "\n",
      "Analyzing user portfolio...\n",
      "\n",
      "User Portfolio Clusters:\n",
      "            Ticker  Cluster\n",
      "0     20MICRONS.NS        0\n",
      "507        INFY.NS        0\n",
      "880    RELIANCE.NS        0\n",
      "1059        TCS.NS        0\n",
      "\n",
      "User's clusters: [0]\n",
      "\n",
      "Clusters without user investment: [1]\n",
      "\n",
      "Fetching company information...\n",
      "\n",
      "Saved 5 stock recommendations to 'recommended_stocks.csv'\n",
      "\n",
      "Recommendations Summary:\n",
      "Number of recommendations per cluster:\n",
      "Cluster\n",
      "1    5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "Motivation:\n",
    "\n",
    "The goal of this module is to enhance portfolio diversification by identifying clusters of stocks based on various financial and market features. \n",
    "\n",
    "The output of this is used in the recommendation module. \n",
    "\n",
    "The module helps us identify the clusters and then recommend the stocks from other clusters which are performing good. All those stocks are taken as input in the recommendation module and then it chooses the stocks from them \n",
    "to give the final output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Output:\n",
    "-------\n",
    "1. A CSV file (`indian_stocks_clusters.csv`) containing the clustering results for each stock.\n",
    "\n",
    "                                                                              \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    " \n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    " \n",
    "# -------------------------------\n",
    "# 1. Load Stock Tickers from CSV\n",
    "# -------------------------------\n",
    " \n",
    "def load_stock_tickers(csv_path=\"updated_sample_indian_stocks_data.csv\"):\n",
    "    \"\"\"\n",
    "    Load stock tickers from a CSV file.\n",
    " \n",
    "    Parameters:\n",
    "        csv_path (str): Path to the CSV file containing stock tickers.\n",
    " \n",
    "    Returns:\n",
    "        list: List of stock tickers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Ensure the 'SYMBOL' column exists\n",
    "        if 'SYMBOL' not in df.columns:\n",
    "            raise ValueError(\"CSV file must contain a 'SYMBOL' column.\")\n",
    "        \n",
    "        # Extract all unique symbols\n",
    "        tickers = df['SYMBOL'].dropna().unique().tolist()\n",
    "        print(f\"Loaded {len(tickers)} tickers from '{csv_path}'.\")\n",
    "        return tickers\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{csv_path}' does not exist.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading tickers: {e}\")\n",
    "        return []\n",
    " \n",
    "# ----------------------------------------\n",
    "# 2. Fetch Stock Data in Batches\n",
    "# ----------------------------------------\n",
    " \n",
    "def fetch_and_save_stock_data(tickers, batch_size=200, start_date='2023-05-01', end_date='2024-11-09', output_csv='sample_indian_stocks_data_full.csv'):\n",
    "    \"\"\"\n",
    "    Fetch stock data in batches and append to a CSV file.\n",
    " \n",
    "    Parameters:\n",
    "        tickers (list): List of stock tickers.\n",
    "        batch_size (int): Number of tickers to process in each batch.\n",
    "        start_date (str): Start date for historical data (YYYY-MM-DD).\n",
    "        end_date (str): End date for historical data (YYYY-MM-DD).\n",
    "        output_csv (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    # Initialize the CSV file with headers if it doesn't exist\n",
    "    if not os.path.exists(output_csv):\n",
    "        headers = ['Ticker', 'Sector', 'Market Cap', 'P/E Ratio', 'Average Return', 'Volatility']\n",
    "        with open(output_csv, 'w') as f:\n",
    "            f.write(','.join(headers) + '\\n')\n",
    "        print(f\"Created '{output_csv}' with headers.\")\n",
    "    else:\n",
    "        print(f\"Appending to existing '{output_csv}'.\")\n",
    " \n",
    "    total_tickers = len(tickers)\n",
    "    total_batches = int(np.ceil(total_tickers / batch_size))\n",
    "    \n",
    "    print(f\"\\nFetching data in {total_batches} batches of {batch_size} tickers each...\\n\")\n",
    "    \n",
    "    for i in tqdm(range(0, total_tickers, batch_size), desc=\"Processing Batches\"):\n",
    "        batch_tickers = tickers[i:i + batch_size]\n",
    "        try:\n",
    "            # Fetch historical price data for the batch\n",
    "            for ticker in batch_tickers:\n",
    "                try:\n",
    "                    print(f\"Processing {ticker}...\")\n",
    "                    stock = yf.Ticker(ticker)\n",
    "                    \n",
    "                    # Fetch historical price data\n",
    "                    hist = stock.history(start=start_date, end=end_date)\n",
    "                    if hist.empty:\n",
    "                        print(f\"No historical data found for {ticker}. Skipping.\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate daily returns\n",
    "                    hist['Daily Return'] = hist['Close'].pct_change()\n",
    "                    avg_return = hist['Daily Return'].mean()\n",
    "                    volatility = hist['Daily Return'].std()\n",
    "                    \n",
    "                    # Fetch financial info\n",
    "                    info = stock.info\n",
    "                    sector = info.get('sector', 'Unknown')\n",
    "                    market_cap = info.get('marketCap', np.nan)\n",
    "                    pe_ratio = info.get('trailingPE', np.nan)\n",
    "                    \n",
    "                    # Prepare row data\n",
    "                    row = {\n",
    "                        'Ticker': ticker,\n",
    "                        'Sector': sector,\n",
    "                        'Market Cap': market_cap if not pd.isna(market_cap) else '',\n",
    "                        'P/E Ratio': pe_ratio if not pd.isna(pe_ratio) else '',\n",
    "                        'Average Return': avg_return,\n",
    "                        'Volatility': volatility\n",
    "                    }\n",
    "                    \n",
    "                    # Append to CSV\n",
    "                    with open(output_csv, 'a') as f:\n",
    "                        f.write(f\"{row['Ticker']},{row['Sector']},{row['Market Cap']},{row['P/E Ratio']},{row['Average Return']},{row['Volatility']}\\n\")\n",
    "                    \n",
    "                    print(f\"Data fetched and saved for {ticker}.\\n\")\n",
    "                    \n",
    "                    # Optional: Sleep to respect API rate limits\n",
    "                    time.sleep(0.1)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {ticker}: {e}\\n\")\n",
    "                    continue\n",
    "            \n",
    "            # Optional: Sleep between batches\n",
    "            time.sleep(1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching batch starting at index {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nData fetching completed. Consolidated data saved to '{output_csv}'.\")\n",
    " \n",
    "# ----------------------------------------\n",
    "# 3. Preprocess the Consolidated Data\n",
    "# ----------------------------------------\n",
    " \n",
    "def preprocess_data(input_csv='sample_indian_stocks_data.csv'):\n",
    "    print(\"\\nPreprocessing data...\")\n",
    " \n",
    "    # Load the consolidated CSV\n",
    "    df = pd.read_csv(input_csv)\n",
    "    print(f\"Loading the latest diversification metrics...\")\n",
    " \n",
    "    # 3.1 Handle Missing Values\n",
    "    # essential_columns = ['Sector', 'Market Cap', 'P/E Ratio', 'Average Return', 'Volatility']\n",
    "    essential_columns = ['Sector', 'Market Cap','Volatility']\n",
    "    df_clean = df.dropna(subset=essential_columns).reset_index(drop=True)\n",
    " \n",
    "    # 3.2 Encode Categorical Variables (Sector) using pd.get_dummies\n",
    "    sector_encoded_df = pd.get_dummies(df_clean['Sector'], prefix='Sector')\n",
    " \n",
    "    # 3.3 Concatenate Encoded Columns with Main DataFrame\n",
    "    df_final = pd.concat([df_clean.drop('Sector', axis=1), sector_encoded_df], axis=1)\n",
    " \n",
    "    # 3.4 Handle any potential NaN or infinite values in the numerical columns\n",
    "    numerical_features = ['Market Cap','Volatility']\n",
    "    \n",
    "    # Replace infinities with NaN, then drop rows with NaN in these columns\n",
    "    df_final[numerical_features] = df_final[numerical_features].replace([np.inf, -np.inf], np.nan)\n",
    " \n",
    "    # Drop rows where any of the numerical features have NaN values\n",
    "    df_final = df_final.dropna(subset=numerical_features)\n",
    " \n",
    "    # 3.5 Feature Scaling\n",
    "    scaler = StandardScaler()\n",
    "    df_final[numerical_features] = scaler.fit_transform(df_final[numerical_features])\n",
    " \n",
    "    print(\"Preprocessing completed.\")\n",
    "    return df_final\n",
    " \n",
    "# ----------------------------------------\n",
    "# 4. Determine Optimal Number of Clusters\n",
    "# ----------------------------------------\n",
    " \n",
    "def determine_optimal_clusters(X, max_k=10):\n",
    "    \"\"\"\n",
    "    Determine the optimal number of clusters using the Elbow Method.\n",
    "\n",
    "    Parameters:\n",
    "        X (pd.DataFrame or np.ndarray): Feature matrix.\n",
    "        max_k (int): Maximum number of clusters to try.\n",
    "\n",
    "    Returns:\n",
    "        int: Optimal number of clusters.\n",
    "    \"\"\"\n",
    "    wcss = []\n",
    "    for k in range(2, max_k + 1):\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
    "        kmeans.fit(X)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    # Elbow method to determine optimal K\n",
    "    optimal_k = 2\n",
    "    for i in range(1, len(wcss) - 1):\n",
    "        if (wcss[i - 1] - wcss[i]) < (wcss[i] - wcss[i + 1]):\n",
    "            optimal_k = i + 2\n",
    "            break\n",
    "\n",
    "    return optimal_k\n",
    "\n",
    " \n",
    "# ----------------------------------------\n",
    "# 5. Perform Clustering\n",
    "# ----------------------------------------\n",
    " \n",
    "def perform_clustering(X, n_clusters):\n",
    "    \"\"\"\n",
    "    Perform K-Means clustering.\n",
    " \n",
    "    Parameters:\n",
    "        X (pd.DataFrame or np.ndarray): Feature matrix.\n",
    "        n_clusters (int): Number of clusters.\n",
    " \n",
    "    Returns:\n",
    "        KMeans: Fitted KMeans object.\n",
    "        np.ndarray: Cluster labels.\n",
    "    \"\"\"\n",
    "    # print(f\"\\nPerforming K-Means clustering with K={n_clusters}...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    # print(\"Clustering completed.\")\n",
    "    return kmeans, labels\n",
    " \n",
    "# ----------------------------------------\n",
    "# 6. Save Clustering Results\n",
    "# ----------------------------------------\n",
    " \n",
    "def save_clustering_results(df, labels, output_path='indian_stocks_clusters.csv'):\n",
    "    \"\"\"\n",
    "    Save the clustering results to a CSV file.\n",
    " \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Preprocessed stock data.\n",
    "        labels (np.ndarray): Cluster labels.\n",
    "        output_path (str): Path to save the CSV file.\n",
    "    \"\"\"\n",
    "    df_with_clusters = df.copy()\n",
    "    df_with_clusters['Cluster'] = labels\n",
    "    df_with_clusters.to_csv(output_path, index=False)\n",
    "    # print(f\"\\nClustering results saved to '{output_path}'.\")\n",
    " \n",
    "# ----------------------------------------\n",
    "# 7. Analyze User Portfolio and Recommend Stocks\n",
    "# ----------------------------------------\n",
    " \n",
    "def analyze_user_portfolio(clustered_df, user_portfolio, top_n=10):\n",
    "    \"\"\"\n",
    "    Analyze the user's portfolio clusters and recommend stocks from other clusters.\n",
    " \n",
    "    Parameters:\n",
    "        clustered_df (pd.DataFrame): DataFrame with clustering results.\n",
    "        user_portfolio (list): List of user's stock tickers.\n",
    "        top_n (int): Number of stock recommendations per cluster.\n",
    " \n",
    "    Returns:\n",
    "        pd.DataFrame: Recommended stocks from clusters the user is not invested in.\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing user portfolio...\")\n",
    " \n",
    "    # Get company info using yfinance for each ticker\n",
    "    def get_company_info(ticker):\n",
    "        try:\n",
    "            stock = yf.Ticker(ticker)\n",
    "            info = stock.info\n",
    "            return {\n",
    "                'Company_Name': info.get('longName', 'N/A'),\n",
    "                'Sector': info.get('sector', 'N/A')\n",
    "            }\n",
    "        except:\n",
    "            return {\n",
    "                'Company_Name': 'N/A',\n",
    "                'Sector': 'N/A'\n",
    "            }\n",
    " \n",
    "    # Filter user's stocks\n",
    "    user_stocks = clustered_df[clustered_df['Ticker'].isin(user_portfolio)]\n",
    " \n",
    "    if user_stocks.empty:\n",
    "        print(\"No matching stocks found in the clustering results for the user portfolio.\")\n",
    "        return pd.DataFrame()\n",
    " \n",
    "    # Display user's stocks and their clusters\n",
    "    print(\"\\nUser Portfolio Clusters:\")\n",
    "    print(user_stocks[['Ticker', 'Cluster']])\n",
    " \n",
    "    # Identify clusters present in user's portfolio\n",
    "    user_clusters = user_stocks['Cluster'].unique()\n",
    "    print(f\"\\nUser's clusters: {user_clusters}\")\n",
    " \n",
    "    # Find clusters where user is not invested\n",
    "    all_clusters = clustered_df['Cluster'].unique()\n",
    "    unrepresented_clusters = [c for c in all_clusters if c not in user_clusters]\n",
    "    print(f\"\\nClusters without user investment: {unrepresented_clusters}\")\n",
    " \n",
    "    if not unrepresented_clusters:\n",
    "        print(\"User has investments in all available clusters.\")\n",
    "        return pd.DataFrame()\n",
    " \n",
    "    # Get recommendations from each unrepresented cluster\n",
    "    recommendations_list = []\n",
    "    \n",
    "    for cluster in unrepresented_clusters:\n",
    "        # Get stocks from this cluster\n",
    "        cluster_stocks = clustered_df[clustered_df['Cluster'] == cluster].copy()\n",
    "        \n",
    "        # Sort by Average Return (descending) and get top N stocks\n",
    "        top_cluster_stocks = cluster_stocks.sort_values(\n",
    "            by='Average Return',\n",
    "            ascending=False\n",
    "        ).head(top_n)\n",
    "        \n",
    "        # Add cluster information\n",
    "        top_cluster_stocks['Recommendation_Reason'] = f'Top performer from Cluster {cluster}'\n",
    "        recommendations_list.append(top_cluster_stocks)\n",
    " \n",
    "    # Combine all recommendations\n",
    "    if recommendations_list:\n",
    "        final_recommendations = pd.concat(recommendations_list, ignore_index=True)\n",
    "        \n",
    "        # Add company information\n",
    "        company_info = []\n",
    "        print(\"\\nFetching company information...\")\n",
    "        for ticker in final_recommendations['Ticker']:\n",
    "            info = get_company_info(ticker)\n",
    "            company_info.append(info)\n",
    "        \n",
    "        # Add new columns\n",
    "        final_recommendations['Company_Name'] = [info['Company_Name'] for info in company_info]\n",
    "        final_recommendations['Sector'] = [info['Sector'] for info in company_info]\n",
    "        \n",
    "        # Select and reorder columns for output\n",
    "        columns_to_keep = [\n",
    "            'Ticker',\n",
    "            'Company_Name',\n",
    "            'Sector',\n",
    "            'Cluster',\n",
    "            'Average Return',\n",
    "            'Volatility',\n",
    "            'Market Cap',\n",
    "            'P/E Ratio',\n",
    "        ]\n",
    "        final_recommendations = final_recommendations[\n",
    "            [col for col in columns_to_keep if col in final_recommendations.columns]\n",
    "        ]\n",
    "        \n",
    "        # Save recommendations to CSV\n",
    "        output_file = 'recommended_stocks.csv'\n",
    "        final_recommendations.to_csv(output_file, index=False)\n",
    "        print(f\"\\nSaved {len(final_recommendations)} stock recommendations to '{output_file}'\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(\"\\nRecommendations Summary:\")\n",
    "        summary = final_recommendations.groupby('Cluster').size()\n",
    "        print(f\"Number of recommendations per cluster:\\n{summary}\")\n",
    "        \n",
    "        return final_recommendations\n",
    "    \n",
    "    return pd.DataFrame()\n",
    " \n",
    "# ----------------------------------------\n",
    "# 8. Visualize Clusters using PCA\n",
    "# ----------------------------------------\n",
    " \n",
    "def plot_clusters(df, n_clusters):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA for dimensionality reduction.\n",
    " \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Preprocessed data with cluster labels.\n",
    "        n_clusters (int): Number of clusters.\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    " \n",
    "    # Separate features and cluster labels\n",
    "    X = df.drop(['Ticker', 'Cluster'], axis=1)\n",
    "    labels = df['Cluster']\n",
    " \n",
    "    # Apply PCA to reduce to 2 dimensions for visualization\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    principal_components = pca.fit_transform(X)\n",
    " \n",
    "    # Create a DataFrame with principal components and cluster labels\n",
    "    pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "    pca_df['Cluster'] = labels\n",
    " \n",
    "    # Plot the clusters\n",
    "    # plt.figure(figsize=(10, 7))\n",
    "    # sns.scatterplot(x='PC1', y='PC2', hue='Cluster', data=pca_df, palette='viridis', s=100, alpha=0.7)\n",
    "    # plt.title('Stock Clusters Visualization using PCA')\n",
    "    # plt.xlabel('Principal Component 1')\n",
    "    # plt.ylabel('Principal Component 2')\n",
    "    # plt.legend(title='Cluster')\n",
    "    # plt.show()\n",
    " \n",
    "# ----------------------------------------\n",
    "# 9. Main Execution Flow\n",
    "# ----------------------------------------\n",
    " \n",
    "def main(sample_user_portfolio):\n",
    "    input_csv = 'sample_indian_stocks_data.csv'\n",
    "    \n",
    "    # Step 1: Preprocess the consolidated data\n",
    "    df_preprocessed = preprocess_data(input_csv=input_csv)\n",
    "    if df_preprocessed.empty:\n",
    "        return\n",
    "    \n",
    "    # Step 2: Determine the optimal number of clusters\n",
    "    X = df_preprocessed.drop(['Ticker'], axis=1)\n",
    "    optimal_k = determine_optimal_clusters(X, max_k=10)\n",
    "    print(f\"Optimal number of clusters determined: {optimal_k}\")\n",
    "    \n",
    "    # Step 3: Perform clustering\n",
    "    kmeans, labels = perform_clustering(X, optimal_k)\n",
    "    \n",
    "    # Step 4: Save clustering results\n",
    "    save_clustering_results(df_preprocessed, labels)\n",
    "    \n",
    "    # Step 5: Analyze user portfolio and recommend stocks\n",
    "    clustered_df = pd.read_csv('indian_stocks_clusters.csv')\n",
    "    recommendations = analyze_user_portfolio(clustered_df, sample_user_portfolio, top_n=10)\n",
    "    if not recommendations.empty:\n",
    "        recommendations.to_csv('recommended_stocks.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_user_portfolio = ['20MICRONS.NS', 'RELIANCE.NS', 'TCS.NS', 'INFY.NS']\n",
    "    main(sample_user_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549f0919-1f77-4a86-ba83-58ba5afef649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (0.2.49)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (1.25.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (3.7.2)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from yfinance) (2.32.2)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from yfinance) (5.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from yfinance) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from yfinance) (2024.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from yfinance) (3.17.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from requests>=2.31->yfinance) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from requests>=2.31->yfinance) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from requests>=2.31->yfinance) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\desktop\\mlsampleproj\\env\\lib\\site-packages (from requests>=2.31->yfinance) (2024.6.2)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/294.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/294.9 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/294.9 kB 245.8 kB/s eta 0:00:02\n",
      "   --------- ----------------------------- 71.7/294.9 kB 357.2 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 71.7/294.9 kB 357.2 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 71.7/294.9 kB 357.2 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 71.7/294.9 kB 357.2 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 143.4/294.9 kB 355.0 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 143.4/294.9 kB 355.0 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 204.8/294.9 kB 414.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 256.0/294.9 kB 462.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 286.7/294.9 kB 491.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- 294.9/294.9 kB 466.7 kB/s eta 0:00:00\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance pandas numpy scikit-learn matplotlib seaborn tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a0384-7fa8-4d75-912f-e23ea47ec670",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
