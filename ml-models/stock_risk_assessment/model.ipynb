{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uilA3K18JbHP",
    "outputId": "8efec4bd-48d1-4a66-dc2b-7c6810275a01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/anaconda3/lib/python3.11/site-packages (7.8.1)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in /opt/anaconda3/lib/python3.11/site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in /opt/anaconda3/lib/python3.11/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/lib/python3.11/site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/anaconda3/lib/python3.11/site-packages (from prawcore<3,>=2.4->praw) (2.31.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.11/site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZH_dr-UXJhT9"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googleapiclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogleapiclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'googleapiclient'"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import yfinance as yf  # Import yfinance to fetch beta\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FCthVUm7Jkz4",
    "outputId": "6c57f1f6-ea02-4865-9b90-8f9c11eeac77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading stocks from /content/recommended_stocks.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to output_stocks_with_names.csv\n",
      "Done!\n",
      "Searching in r/stocks for '$RELIANCE'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in r/stocks for 'Reliance Industries'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in r/investing for '$RELIANCE'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in r/investing for 'Reliance Industries'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in r/IndianStockMarket for '$RELIANCE'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching in r/IndianStockMarket for 'Reliance Industries'...\n",
      "\n",
      "Fetched 60 Reddit posts.\n",
      "\n",
      "Reddit sentiment results saved to 'reddit_stock_sentiment_adjusted_scores.csv'.\n",
      "\n",
      "Searching YouTube for 'RELIANCE'...\n",
      "Found 5 videos for query 'RELIANCE'.\n",
      "\n",
      "Searching YouTube for 'Reliance Industries'...\n",
      "Found 5 videos for query 'Reliance Industries'.\n",
      "\n",
      "Total videos found: 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n",
      "WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"commentsDisabled\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching comments for video 0M9fdsBZdDs: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?videoId=0M9fdsBZdDs&part=snippet&maxResults=10&textFormat=plainText&order=relevance&key=AIzaSyD-QvEK0JfHRzqYdlympWADm3yVuJj32Gg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "Error fetching comments for video Zy9mRqp6IP4: <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/commentThreads?videoId=Zy9mRqp6IP4&part=snippet&maxResults=10&textFormat=plainText&order=relevance&key=AIzaSyD-QvEK0JfHRzqYdlympWADm3yVuJj32Gg&alt=json returned \"The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.\". Details: \"[{'message': 'The video identified by the <code><a href=\"/youtube/v3/docs/commentThreads/list#videoId\">videoId</a></code> parameter has disabled comments.', 'domain': 'youtube.commentThread', 'reason': 'commentsDisabled', 'location': 'videoId', 'locationType': 'parameter'}]\">\n",
      "\n",
      "Fetched and analyzed 0 relevant YouTube comments.\n",
      "\n",
      "YouTube sentiment results saved to 'youtube_stock_sentiment.csv'.\n",
      "\n",
      "\n",
      "Final Risk Score for RELIANCE.NS: 0.56\n",
      "\n",
      "Reddit Results                                         Combined_Text  Adjusted_Score\n",
      "0   Investing in INDA & SMIN, hoping for Indian ET...        0.000000\n",
      "1   Future of Energy Stocks  So I've been doing so...       -0.349175\n",
      "2   Lyft Achieves Record-Breaking Q3 2024 with Sur...        0.000000\n",
      "3   The Overlooked Risk in AI Stocks: It Has to Do...        0.000000\n",
      "4   What’s going on with Novo Nordisk (NVO)? The s...        0.000000\n",
      "5   The complexity of Bio stock. $IBRX  ImmunityBi...        0.000000\n",
      "6   What could stop the NVDIA frenzy? [What could ...        0.384395\n",
      "7   Alibaba shares dip 3% in premarket after earni...        0.452627\n",
      "8   Advice on Junior ISA regarding recession fears...       -0.450024\n",
      "9   My thoughts on META **I'd like to start a dial...        0.000000\n",
      "10  Investing in INDA & SMIN, hoping for Indian ET...        0.000000\n",
      "11  What’s going on with Novo Nordisk (NVO)? The s...        0.000000\n",
      "12  India sends 100 antitrust queries for Reliance...        0.508439\n",
      "13  Disney (DIS) DCF Analysis # Introduction:\\n\\nD...       -0.367379\n",
      "14  Amazon (AMZN) DCF Analysis ***NOTE NOT FINANCI...        0.410755\n",
      "15  Paramount Global Stock Pops On Apollo’s Report...        0.534216\n",
      "16  Foot Locker shares fall after heavy promotions...        0.589956\n",
      "17  Disney Press Release: Disney India & Reliance ...        0.403350\n",
      "18  Disney & Reliance in India sign a binding agre...        0.448567\n",
      "19  Macro report: Freight Cost Increases Due to Re...        0.000000\n",
      "20  Additional details on upcoming tariffs I’ve se...        0.000000\n",
      "21  Some publicly traded agriculture companies lik...       -0.292851\n",
      "22  Pfizer Investment Analysis: Strong Q3 Performa...        0.000000\n",
      "23  Creating a Python library for Mandelbrot's met...        0.000000\n",
      "24  What are your expectations for green/renewable...        0.000000\n",
      "25  Next Steps with Rebalance and Growth I posted ...        0.000000\n",
      "26  Investing in India as a next growth powerhouse...        0.000000\n",
      "27  Advice - Nvidea allocation Currently i have 17...        0.000000\n",
      "28  gold - why is it rising? Dollar remains strong...       -0.280600\n",
      "29  China Urges EV Makers to Buy Local Chips as US...        0.403055\n",
      "30  Pfizer Investment Analysis: Strong Q3 Performa...        0.000000\n",
      "31  Investing in India as a next growth powerhouse...        0.000000\n",
      "32  China Urges EV Makers to Buy Local Chips as US...        0.403055\n",
      "33  ASE Technology Holding Co (ASX) - nearing 52 w...        0.000000\n",
      "34  Auditors Didn’t Flag Risks Building Up in Bank...        0.431770\n",
      "35  U.S Index Funds are the Way to Go \\>Positive b...        0.000000\n",
      "36  Stumped on my 8th stock to pick in my portfoli...       -0.317941\n",
      "37  Intel Corp. - INTC Evaluation - Adjustment of ...        0.000000\n",
      "38  Intel Corporation Evaluation - Comparison with...        0.000000\n",
      "39  Intel Analysis - Serious DD ***This is not a f...        0.404363\n",
      "40  Trying to understand slippage % for intraday e...        0.000000\n",
      "41  Index movement - Bank Nifty and Nifty Been not...        0.000000\n",
      "42  Mutual fund to stock market move I have invest...        0.000000\n",
      "43  Is Reliance worth it at this level? https://pr...        0.298329\n",
      "44  Avenue Supermart Q2 FY25 Result Updates Discla...        0.326811\n",
      "45  DuPont Analysis: Digging Deeper into Return on...        0.000000\n",
      "46  Stocks to buy during this dip?  Like im intere...       -0.382151\n",
      "47  Is Reliance Ind stock taking a beating due to ...       -0.400465\n",
      "48  Vedanta Demerger - Chairman Message [Chairman ...        0.000000\n",
      "49  Longterm Investing Im investing in Mutual fund...        0.000000\n",
      "50  Is Reliance worth it at this level? https://pr...        0.298329\n",
      "51  [UPDATE] Monthly Analysis of Quant Small Cap F...       -0.270752\n",
      "52  What just happened with Reliance industries? I...        0.574262\n",
      "53  Investment Strategy: Seeking Advice on Adding ...       -0.291998\n",
      "54  Can someone help me understand what this messa...        0.665864\n",
      "55  Reliance shares bonus issue  I received a mail...       -0.290270\n",
      "56  What does this mean? https://preview.redd.it/6...        0.346872\n",
      "57  1st time seeing this type of message by CDSL W...        0.303915\n",
      "58  How this stock went from 3.5rs to 2.3 Lakh, ca...        0.470542\n",
      "59  Why Reliance Industries share price have dropp...        0.445688 , Youtube Results Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logs\n",
    "\n",
    "# Suppress NLTK download output\n",
    "original_stdout = sys.stdout\n",
    "sys.stdout = io.StringIO()\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "sys.stdout = original_stdout\n",
    "\n",
    "# Suppress other library logs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================\n",
    "# 1. Setup and Load Pre-trained BERT Model\n",
    "# =============================================\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# =============================================\n",
    "# 2. Define Sentiment Prediction Function\n",
    "# =============================================\n",
    "\n",
    "def predict_sentiment(text, tokenizer, model, device):\n",
    "    \"\"\"\n",
    "    Predicts the sentiment of a given text using a pre-trained BERT model.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "        tokenizer: The BERT tokenizer.\n",
    "        model: The pre-trained BERT model.\n",
    "        device: The device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (Sentiment Label, Confidence Score)\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return 'neutral', 0.0\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        confidence, predicted_class = torch.max(probabilities, dim=1)\n",
    "\n",
    "    # Map numerical labels to sentiment\n",
    "    label_mapping = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "    sentiment = label_mapping.get(predicted_class.item(), 'Neutral')\n",
    "    confidence_score = confidence.item()\n",
    "\n",
    "    return sentiment, confidence_score\n",
    "\n",
    "# =============================================\n",
    "# 3. Reddit Sentiment Analysis Function\n",
    "# =============================================\n",
    "\n",
    "def reddit_sentiment_analysis(client_id, client_secret, user_agent, stock_ticker, company_name, subreddits, reddit_limit=100):\n",
    "    \"\"\"\n",
    "    Fetches Reddit posts related to the stock and performs sentiment analysis.\n",
    "\n",
    "    Parameters:\n",
    "        client_id (str): Reddit API client ID.\n",
    "        client_secret (str): Reddit API client secret.\n",
    "        user_agent (str): Reddit API user agent.\n",
    "        stock_ticker (str): Stock ticker symbol (e.g., 'RELIANCE.NS').\n",
    "        company_name (str): Full company name (e.g., 'Reliance Industries').\n",
    "        subreddits (list): List of subreddit names to search.\n",
    "        reddit_limit (int): Number of posts to fetch per query per subreddit.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DataFrame containing comments and adjusted scores, average adjusted score)\n",
    "    \"\"\"\n",
    "    # Initialize Reddit instance\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent\n",
    "    )\n",
    "\n",
    "    queries = [f\"${stock_ticker.split('.')[0]}\", company_name]\n",
    "    posts = []\n",
    "\n",
    "    for subreddit in subreddits:\n",
    "        for query in queries:\n",
    "            print(f\"Searching in r/{subreddit} for '{query}'...\")\n",
    "            try:\n",
    "                for submission in reddit.subreddit(subreddit).search(query, limit=reddit_limit, sort='new'):\n",
    "                    posts.append({\n",
    "                        'Combined_Text': f\"{submission.title} {submission.selftext}\"\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching posts from r/{subreddit} with query '{query}': {e}\")\n",
    "                continue\n",
    "\n",
    "    reddit_df = pd.DataFrame(posts)\n",
    "    print(f\"\\nFetched {len(reddit_df)} Reddit posts.\\n\")\n",
    "\n",
    "    # Perform sentiment analysis\n",
    "    adjusted_scores = []\n",
    "\n",
    "    for text in tqdm(reddit_df['Combined_Text'], desc=\"Analyzing Reddit Sentiments\", disable=True):\n",
    "        sentiment, confidence = predict_sentiment(text, tokenizer, model, device)\n",
    "\n",
    "        # Assign numeric value based on sentiment\n",
    "        if sentiment == 'Positive':\n",
    "            sentiment_value = -1\n",
    "        elif sentiment == 'Neutral':\n",
    "            sentiment_value = 0\n",
    "        else:  # Negative\n",
    "            sentiment_value = 1\n",
    "\n",
    "        # Calculate adjusted score\n",
    "        adjusted_score = sentiment_value * confidence\n",
    "        adjusted_scores.append(adjusted_score)\n",
    "\n",
    "    # Add adjusted scores to DataFrame\n",
    "    reddit_df['Adjusted_Score'] = adjusted_scores\n",
    "\n",
    "    # Calculate the average of adjusted scores\n",
    "    average_adjusted_score = reddit_df['Adjusted_Score'].mean()\n",
    "\n",
    "    # Filter to only keep the relevant column\n",
    "    reddit_output_df = reddit_df[['Combined_Text', 'Adjusted_Score']]\n",
    "\n",
    "    # Save to CSV\n",
    "    reddit_output_df.to_csv('reddit_stock_sentiment_adjusted_scores.csv', index=False)\n",
    "    print(\"Reddit sentiment results saved to 'reddit_stock_sentiment_adjusted_scores.csv'.\\n\")\n",
    "\n",
    "    return reddit_output_df, average_adjusted_score\n",
    "\n",
    "# =============================================\n",
    "# 4. YouTube Sentiment Analysis Function\n",
    "# =============================================\n",
    "\n",
    "def youtube_sentiment_analysis(api_key, stock_symbol, company_name, youtube_limit=5, comments_limit=100):\n",
    "    \"\"\"\n",
    "    Fetches YouTube comments related to the stock and performs sentiment analysis.\n",
    "\n",
    "    Parameters:\n",
    "        api_key (str): YouTube Data API key.\n",
    "        stock_symbol (str): Stock symbol (e.g., 'RELIANCE.NS').\n",
    "        company_name (str): Full company name (e.g., 'Reliance Industries').\n",
    "        youtube_limit (int): Number of videos to fetch per query.\n",
    "        comments_limit (int): Number of comments to fetch per video.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing comments and their sentiment scores.\n",
    "    \"\"\"\n",
    "    search_queries = [f\"{stock_symbol.split('.')[0]}\", company_name]\n",
    "    video_ids = []\n",
    "\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    for query in search_queries:\n",
    "        print(f\"Searching YouTube for '{query}'...\")\n",
    "        try:\n",
    "            search_response = youtube.search().list(\n",
    "                q=query,\n",
    "                part='id',\n",
    "                type='video',\n",
    "                maxResults=youtube_limit,\n",
    "                order='date'  # Fetch recent videos\n",
    "            ).execute()\n",
    "\n",
    "            vids = [item['id']['videoId'] for item in search_response.get('items', [])]\n",
    "            video_ids.extend(vids)\n",
    "            print(f\"Found {len(vids)} videos for query '{query}'.\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching YouTube for query '{query}': {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"Total videos found: {len(video_ids)}\\n\")\n",
    "\n",
    "    # Fetch comments for each video\n",
    "    all_comments = []\n",
    "    for vid in tqdm(video_ids, desc=\"Fetching YouTube Comments\", disable=True):\n",
    "        try:\n",
    "            comment_response = youtube.commentThreads().list(\n",
    "                videoId=vid,\n",
    "                part='snippet',\n",
    "                maxResults=comments_limit,\n",
    "                textFormat='plainText',\n",
    "                order='relevance'\n",
    "            ).execute()\n",
    "\n",
    "            for item in comment_response.get('items', []):\n",
    "                comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                # Check if the comment mentions the stock symbol or company name\n",
    "                if stock_symbol.split('.')[0].lower() in comment.lower() or company_name.lower() in comment.lower():\n",
    "                    sentiment, confidence = predict_sentiment(comment, tokenizer, model, device)\n",
    "                    all_comments.append({\n",
    "                        'Comment': comment,\n",
    "                        'Sentiment': sentiment,\n",
    "                        'Confidence': confidence\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching comments for video {vid}: {e}\")\n",
    "            continue\n",
    "        # To respect API rate limits\n",
    "        time.sleep(1)\n",
    "\n",
    "    youtube_df = pd.DataFrame(all_comments)\n",
    "    print(f\"\\nFetched and analyzed {len(youtube_df)} relevant YouTube comments.\\n\")\n",
    "\n",
    "    # Save to CSV\n",
    "    youtube_df.to_csv('youtube_stock_sentiment.csv', index=False)\n",
    "    print(\"YouTube sentiment results saved to 'youtube_stock_sentiment.csv'.\\n\")\n",
    "\n",
    "    return youtube_df\n",
    "\n",
    "# =============================================\n",
    "# 5. Main Function\n",
    "# =============================================\n",
    "\n",
    "def for_one_stock(lele,lelecompany_name):\n",
    "    # ----------------------------------------\n",
    "    # User Inputs\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Reddit API Credentials\n",
    "    reddit_client_id = 'YS3tFotOu28fhCV6qbn9Ag'          # Replace with your client ID\n",
    "    reddit_client_secret = 'XNgpl9XAxCo326LKUgYKYFMK40o0eA' # Replace with your client secret\n",
    "    reddit_user_agent = 'stock_sentiment_analysis by /u/Comfortable-Title817'  # Replace with your Reddit username\n",
    "\n",
    "    # YouTube API Key\n",
    "    youtube_api_key = 'AIzaSyD-QvEK0JfHRzqYdlympWADm3yVuJj32Gg'  # Replace with your actual YouTube Data API key\n",
    "\n",
    "    # Stock Information\n",
    "    stock_ticker = lele # Example: RELIANCE Industries\n",
    "    company_name = lelecompany_name\n",
    "\n",
    "    # Subreddits to search\n",
    "    subreddits = ['stocks', 'investing', 'IndianStockMarket']\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Reddit Sentiment Analysis\n",
    "    # ----------------------------------------\n",
    "\n",
    "    reddit_results, average_sentiment_score = reddit_sentiment_analysis(\n",
    "        client_id=reddit_client_id,\n",
    "        client_secret=reddit_client_secret,\n",
    "        user_agent=reddit_user_agent,\n",
    "        stock_ticker=stock_ticker,\n",
    "        company_name=company_name,\n",
    "        subreddits=subreddits,\n",
    "        reddit_limit=10  # Number of posts per query per subreddit\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # YouTube Sentiment Analysis (Optional)\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Uncomment the following lines if you want to perform YouTube sentiment analysis\n",
    "    youtube_results = youtube_sentiment_analysis(\n",
    "        api_key=youtube_api_key,\n",
    "        stock_symbol=stock_ticker,\n",
    "        company_name=company_name,\n",
    "        youtube_limit=5,    # Number of videos per query\n",
    "        comments_limit=10   # Number of comments per video\n",
    "    )\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Fetch Beta Value from yFinance\n",
    "    # ----------------------------------------\n",
    "\n",
    "    try:\n",
    "        ticker_obj = yf.Ticker(stock_ticker)\n",
    "        beta = ticker_obj.info.get('beta', None)\n",
    "        if beta is None:\n",
    "            print(\"Beta value not found for the stock.\")\n",
    "            beta = 0.0  # Assign a default value or handle accordingly\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching beta from yFinance: {e}\")\n",
    "        beta = 0.0  # Assign a default value or handle accordingly\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Calculate Final Risk Score\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Change the sign of the sentiment score\n",
    "    adjusted_sentiment_score = -average_sentiment_score\n",
    "\n",
    "    # Calculate weighted average: 80% beta and 20% adjusted sentiment score\n",
    "    final_risk_score = 0.9 * beta + 0.1 * adjusted_sentiment_score\n",
    "\n",
    "    # Determine if it's an increase or decrease\n",
    "    if final_risk_score > 0:\n",
    "        risk_trend = \"Increase\"\n",
    "    elif final_risk_score < 0:\n",
    "        risk_trend = \"Decrease\"\n",
    "    else:\n",
    "        risk_trend = \"No Change\"\n",
    "\n",
    "    # ----------------------------------------\n",
    "    # Print Final Risk Score\n",
    "    # ----------------------------------------\n",
    "\n",
    "    print(f\"\\nFinal Risk Score for {stock_ticker}: {final_risk_score:.2f}\")\n",
    "    print(f\"\\nReddit Results {reddit_results} , Youtube Results {youtube_results}\")\n",
    "\n",
    "    # Optionally, you can merge or further process these results as needed\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "# for_one_stock('RELIANCE.NS','Reliance Industries')\n",
    "# Add this code at the end of your existing file, after the for_one_stock function\n",
    "\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Function to fetch company name from ticker\n",
    "def get_company_name(ticker):\n",
    "    try:\n",
    "        stock = yf.Ticker(ticker)\n",
    "        return stock.info.get('longName', 'Unknown')  # Returns the company name\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching company name for {ticker}: {str(e)}\")\n",
    "        return 'Unknown'\n",
    "\n",
    "def add_company_names(input_csv_path, output_csv_path):\n",
    "    \"\"\"\n",
    "    Add company names to the input CSV based on ticker symbols and save the results to output CSV.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV containing recommended stocks\n",
    "        output_csv_path (str): Path to save the output CSV with company names added\n",
    "    \"\"\"\n",
    "    # Read the input CSV\n",
    "    print(f\"Reading stocks from {input_csv_path}\")\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Add the 'Company_Name' column\n",
    "    df['Company_Name'] = df['Ticker'].apply(get_company_name)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV\n",
    "    print(f\"Saving results to {output_csv_path}\")\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(\"Done!\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = \"/content/recommended_stocks.csv\"  # Path to your input CSV file\n",
    "output_csv_path = \"output_stocks_with_names.csv\"  # Path to save the new CSV\n",
    "add_company_names(input_csv_path, output_csv_path)\n",
    "for_one_stock('RELIANCE.NS','Reliance Industries')\n",
    "\n",
    "# # Add this at the very end of your file\n",
    "# if __name__ == \"__main__\":\n",
    "#     input_path = \"/content/recommended_stocks.csv\"  # Path to your input CSV\n",
    "#     output_path = \"recommended_stocks_with_risk.csv\"  # Path for output CSV\n",
    "\n",
    "#     calculate_risk_scores(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HA11n3VH-r4r"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
